{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iJIaFzs3_5JI",
        "W2Emyy8JEfYC",
        "uLdnuNJR8QAm"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1fdND6DyEHPI+xds74Ayc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4rldur0/whyfi/blob/xeoyeon/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. ì´ˆê¸° ì„¤ì •"
      ],
      "metadata": {
        "id": "cC9rzq7eai-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMfHySIj6nnG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/RAG"
      ],
      "metadata": {
        "id": "UnRtUY0U61M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers chromadb langchain langchain_community langchain-chroma"
      ],
      "metadata": {
        "id": "uRlMWYW28arl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. data ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "iJIaFzs3_5JI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/RAG/dataset/cleaned_word_dict.csv'\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "iPg9guWzFg2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pdf ê°€ì ¸ì™€ì„œ chunkí•˜ê¸°"
      ],
      "metadata": {
        "id": "W2Emyy8JEfYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "file_name = # ì½ì–´ì˜¤ë ¤ëŠ” íŒŒì¼ ê²½ë¡œ\n",
        "\n",
        "loader = PyPDFLoader(file_name)\n",
        "pages = loader.load()\n",
        "text = \"\"\n",
        "for page in pages:\n",
        "    sub = page.page_content\n",
        "    text += sub"
      ],
      "metadata": {
        "id": "Ww-X-VFtEim9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# token size ê¸°ì¤€ìœ¼ë¡œ contents split\n",
        "tokenizer = AutoTokenizer.from_pretrained(ENCODER)\n",
        "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
        "            tokenizer,\n",
        "            chunk_size=CHUNK_SIZE,\n",
        "            chunk_overlap=CHUNK_OVERLAP,\n",
        "            separator=\"\\n\" # default: \"\\n\\n\"\n",
        "        )\n",
        "\n",
        "\n",
        "documents=[] # split í•œ ë¬¸ì„œë“¤ì„ ë‹´ê¸° ìœ„í•œ array\n",
        "\n",
        "split_conts = text_splitter.split_text(text)\n",
        "for chunk_idx, split_cont in enumerate(split_conts):\n",
        "    documents.append(Document(\n",
        "        page_content=split_cont,\n",
        "        metadata={\n",
        "            \"file_name\": file_name,\n",
        "        },\n",
        "        id=chunk_idx,\n",
        "    ))\n",
        "    idx+=1\n",
        "\n",
        "vectorstore.add_documents(documents)"
      ],
      "metadata": {
        "id": "jcKE3UxdEk0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## website ë¬¸ì„œ ë¡œë”©í•˜ê¸°"
      ],
      "metadata": {
        "id": "uLdnuNJR8QAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load documents from web\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "web_loader = WebBaseLoader([\n",
        "    \"ë§í¬ ì‚½ì…\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "data = web_loader.load()"
      ],
      "metadata": {
        "id": "eRgMGULC_8nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Split documents into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0\n",
        ")\n",
        "\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "\n",
        "all_splits[0]\n",
        "ì¶œì²˜: https://rfriend.tistory.com/832 [R, Python ë¶„ì„ê³¼ í”„ë¡œê·¸ë˜ë°ì˜ ì¹œêµ¬ (by R Friend):í‹°ìŠ¤í† ë¦¬]"
      ],
      "metadata": {
        "id": "WXP97Wk7AOnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. chromaDBì— ë°ì´í„° ì €ì¥í•˜ê¸°"
      ],
      "metadata": {
        "id": "EJllvlFiAUVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface"
      ],
      "metadata": {
        "id": "7ZFBiU88C91n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ê¸°ì¡´ ì„ë² ë”© ê²°ê³¼ë§Œ ê°€ì ¸ì˜¤ê¸°\n",
        "import os\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embedding =\"dragonkue/BGE-m3-ko\"\n",
        "collection_name = \"Chroma_Collection\"\n",
        "chroma_path = \"/content/drive/MyDrive/Colab Notebooks/RAG\" #ë°ì´í„° ì €ì¥ ê²½ë¡œ\n",
        "\n",
        "# Init chromadb\n",
        "embedding_func = HuggingFaceEmbeddings(model_name=embedding, encode_kwargs={'normalize_embeddings':True},)\n",
        "vectorstore = Chroma(\n",
        "    collection_name,\n",
        "    embedding_function=embedding_func,\n",
        "    persist_directory=chroma_path,\n",
        "    collection_metadata={\"max_size\": 1000}  # ìš©ëŸ‰ ì„¤ì • (1000ê°œë¡œ í™•ì¥)\n",
        ")\n"
      ],
      "metadata": {
        "id": "8q3EifDs_3Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ\n",
        "vectorstore._client.delete_collection(\"Chroma_Collection\")\n",
        "\n",
        "# ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ ìƒì„±\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"Chroma_Collection\",\n",
        "    embedding_function=embedding_func,\n",
        "    persist_directory=\"/content/drive/MyDrive/Colab Notebooks/RAG\",\n",
        "    collection_metadata={\"max_size\": 1000}\n",
        ")\n"
      ],
      "metadata": {
        "id": "Dmi5KRbtLUrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document # ë°ì´í„°ì˜ ê° rowë¥¼ document ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ê¸° ìœ„í•¨\n",
        "\n",
        "# vectorDBì— data ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜\n",
        "def add_data_to_vectorstore(data, vectorstore):\n",
        "  for index, row in data.iterrows(): # rowë¼ëŠ” ë³€ìˆ˜ì— ê° í–‰ì„ ë°˜ë³µì ìœ¼ë¡œ ê°€ì ¸ì˜´.\n",
        "    text = row.get(\"Content\",\"\")\n",
        "    metadata = row.to_dict() #í–‰ ì „ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜\n",
        "    metadata[\"source\"] = metadata.get(\"source\", f\"row_{index}\")   # source í•„ë“œ ì¶”ê°€ (ê¸°ë³¸ê°’ìœ¼ë¡œ í–‰ ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ íŠ¹ì • ì—´ì—ì„œ ê°€ì ¸ì˜¤ê¸°)\n",
        "    document = Document(page_content=text, metadata=metadata) #cocument ê°ì²´ë¥¼ ìƒì„±\n",
        "    vectorstore.add_texts([document.page_content],[document.metadata]) # dbì— ë°ì´í„° ì¶”ê°€\n",
        "\n",
        "add_data_to_vectorstore(data, vectorstore)"
      ],
      "metadata": {
        "id": "6MjvPo_OG4j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Retriever ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •"
      ],
      "metadata": {
        "id": "pydocZFnbP4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a retriever to search in the vectorstore\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) #ê²€ìƒ‰ ì‹œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ 3ê°œì˜ ë¬¸ì„œë¥¼ ë°˜í™˜í•˜ë¼ëŠ” ëœ»"
      ],
      "metadata": {
        "id": "z0WAxsuOIonS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# system_template=\"\"\"\n",
        "# You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "# You MUST answer in Korean.\n",
        "\n",
        "# Question: {question}\n",
        "# Context: {context}\n",
        "# Answer:\n",
        "# \"\"\"\n",
        "system_template = \"\"\"ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ë¡œ, ë³µì¡í•œ ê¸ˆìœµ ìš©ì–´ë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ë° ëŠ¥ìˆ™í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ê¸ˆìœµê³¼ ê´€ë ¨ëœ ìš©ì–´ì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´, ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "ìš©ì–´ ì„¤ëª…: ì§ˆë¬¸ì— í¬í•¨ëœ ê¸ˆìœµ ìš©ì–´ë¥¼ ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì–¸ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n",
        "ì¶”ê°€ ì •ë³´: ì‚¬ìš©ìê°€ í•´ë‹¹ ê°œë…ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡, ì‹¤ì œ ì‚¬ë¡€ë‚˜ ë¹„ìœ ë¥¼ í¬í•¨í•˜ì—¬ ì„¤ëª…ì„ ë³´ì™„í•˜ì„¸ìš”.\n",
        "ê´€ë ¨ ìš©ì–´: í•´ë‹¹ ìš©ì–´ì™€ ì—°ê´€ëœ ë‹¤ë¥¸ ê¸ˆìœµ ìš©ì–´ë‚˜ ê°œë…ì„ ìµœëŒ€ 3ê°œê¹Œì§€ ì¶”ì²œí•˜ì„¸ìš”.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(messages)"
      ],
      "metadata": {
        "id": "Ry-kcYg3M7ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. gemini api ì„¤ì • ë° chain êµ¬í˜„í•˜ê¸°"
      ],
      "metadata": {
        "id": "Y29G7jl4F1Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-vertexai"
      ],
      "metadata": {
        "id": "VfkpxLXDTZQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"your_project_id\"\n",
        "REGION = \"your_region\"\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "rmnNDrjsUCXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain_google_vertexai import VertexAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "vertexai.init(project = PROJECT_ID , location = REGION)\n",
        "\n",
        "chain_type_kwargs = {\n",
        "    \"prompt\": prompt,\n",
        "    \"document_variable_name\": \"context\",  # 'context'ê°€ documentsë¥¼ ë°›ì„ ë³€ìˆ˜ì„ì„ ëª…ì‹œ\n",
        "}\n",
        "llm = VertexAI(\n",
        "    temperature=0,\n",
        "    model_name=\"gemini-pro\",\n",
        "    max_output_tokens=1024\n",
        ")\n",
        "\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever = retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "UshxPJxgNiU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain ì‚¬ìš© ì˜ˆì‹œ\n",
        "question = \"ê°€ë™ë¥ ì´ë€?\"\n",
        "result = chain({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", result[\"answer\"])\n",
        "print(\"Sources:\", result[\"source_documents\"])"
      ],
      "metadata": {
        "id": "GGimgZgpYGpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. streamlit ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ì›¹ ui êµ¬í˜„í•˜ê¸°\n",
        "1. [ngrok](https://dashboard.ngrok.com/) ì— ì ‘ì† í›„ íšŒì›ê°€ì…\n",
        "2. ë¡œê·¸ì¸ í›„ ëœ¨ëŠ” authtoken ë²ˆí˜¸ë¥¼ ì•„ë˜ì˜ ì½”ë“œì— ë¶™ì—¬ë„£ê¸°\n"
      ],
      "metadata": {
        "id": "h_plpPTTGzhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "w43K67gkG6QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import vertexai\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain_google_vertexai import VertexAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import re\n",
        "\n",
        "embedding =\"dragonkue/BGE-m3-ko\"\n",
        "collection_name = \"Chroma_Collection\"\n",
        "chroma_path = \"/content/drive/MyDrive/Colab Notebooks/RAG\" #ë°ì´í„° ì €ì¥ ê²½ë¡œ\n",
        "\n",
        "# Init chromadb\n",
        "embedding_func = HuggingFaceEmbeddings(model_name=embedding, encode_kwargs={'normalize_embeddings':True},)\n",
        "vectorstore = Chroma(\n",
        "    collection_name,\n",
        "    embedding_function=embedding_func,\n",
        "    persist_directory=chroma_path,\n",
        "    collection_metadata={\"max_size\": 1000}  # ìš©ëŸ‰ ì„¤ì • (1000ê°œë¡œ í™•ì¥)\n",
        ")\n",
        "\n",
        "#retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "#ì´ˆê¸°í™”\n",
        "system_template = \"\"\"ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ë¡œ, ë³µì¡í•œ ê¸ˆìœµ ìš©ì–´ë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ë° ëŠ¥ìˆ™í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ê¸ˆìœµê³¼ ê´€ë ¨ëœ ìš©ì–´ì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´, ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "ìš©ì–´ ì„¤ëª…: ì§ˆë¬¸ì— í¬í•¨ëœ ê¸ˆìœµ ìš©ì–´ë¥¼ ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì–¸ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n",
        "ì¶”ê°€ ì •ë³´: ì‚¬ìš©ìê°€ í•´ë‹¹ ê°œë…ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡, ì‹¤ì œ ì‚¬ë¡€ë‚˜ ë¹„ìœ ë¥¼ í¬í•¨í•˜ì—¬ ì„¤ëª…ì„ ë³´ì™„í•˜ì„¸ìš”.\n",
        "ê´€ë ¨ ìš©ì–´: í•´ë‹¹ ìš©ì–´ì™€ ì—°ê´€ëœ ë‹¤ë¥¸ ê¸ˆìœµ ìš©ì–´ë¥¼ ì¶”ì²œí•˜ì„¸ìš”.\n",
        "\n",
        "ë‹¨, ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì•¼ í•˜ë©°, ì „ì²´ Answerì˜ ê¸¸ì´ê°€ 8ì¤„ì„ ë„˜ì–´ê°€ì§€ ì•Šë„ë¡ í•˜ì„¸ìš”.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "#gemini\n",
        "PROJECT_ID = \"your_project_id\"\n",
        "REGION = \"your_region\"\n",
        "\n",
        "vertexai.init(project = PROJECT_ID , location = REGION)\n",
        "\n",
        "chain_type_kwargs = {\n",
        "    \"prompt\": prompt,\n",
        "    \"document_variable_name\": \"context\",  # 'context'ê°€ documentsë¥¼ ë°›ì„ ë³€ìˆ˜ì„ì„ ëª…ì‹œ\n",
        "}\n",
        "llm = VertexAI(\n",
        "    temperature=0,\n",
        "    model_name=\"gemini-pro\",\n",
        "    max_output_tokens=1024\n",
        ")\n",
        "\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever = retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs\n",
        ")\n",
        "\n",
        "#streamlit ì„¤ê³„\n",
        "st.set_page_config(page_title=\"ê¸ˆìœµìš©ì–´ì•Œë¦¬ë¯¸ : WhyFi\")\n",
        "\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <h1 style='color: white; text-align: center;'>ê¸ˆìœµìš©ì–´ì•Œë¦¬ë¯¸ : WhyFi</h1>\n",
        "    <h2 style='color: gray; text-align: center; font-size: 20px;'>ëª¨ë¥´ëŠ” ê¸ˆìœµ ìš©ì–´? ì´ì œ ì‰½ê²Œ ì°¾ì•„ë³´ì„¸ìš”!</h2>\n",
        "    <br>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "\n",
        "user_question = st.text_input(\"ê¸ˆìœµ ìš©ì–´ë¥¼ ê²€ìƒ‰í•´ë³´ì„¸ìš”.\", value=\"\", placeholder=\"ì˜ˆ: ê°€ë™ë¥ ì´ë€?\")\n",
        "if user_question:\n",
        "    with st.spinner(\"ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\"):\n",
        "        result = chain({\"question\": user_question})\n",
        "\n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    st.markdown(\"### ğŸ“– ë‹µë³€ ğŸ“–\")\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ì—ì„œ í—¤ë”(##)ë¥¼ ë¶„ë¦¬í•˜ê³  ìŠ¤íƒ€ì¼ ì ìš©\n",
        "    answer_text = result[\"answer\"]\n",
        "\n",
        "    # í—¤ë”ì™€ ë³¸ë¬¸ ë¶„ë¦¬\n",
        "    header_match = re.match(r\"^##\\s*(.*)\", answer_text)\n",
        "    if header_match:\n",
        "        header = header_match.group(1)  # í—¤ë” ë¶€ë¶„\n",
        "        body = re.sub(r\"^##\\s*.*\\n*\", \"\", answer_text, flags=re.MULTILINE)  # ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸\n",
        "    else:\n",
        "        header = \"\"\n",
        "        body = answer_text\n",
        "\n",
        "    # í—¤ë”ì™€ ë³¸ë¬¸ ê°ê° ìŠ¤íƒ€ì¼ë§\n",
        "    st.markdown(\n",
        "        f\"<div style='font-size:20px; font-weight:bold; line-height:1.8;'>{header}</div>\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "    st.markdown(\n",
        "        f\"<div style='font-size:16px; line-height:1.6;'>{body}</div>\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "    st.markdown(\"### ğŸ“š ì°¸ê³  ì¶œì²˜ ğŸ“š\")\n",
        "    for doc in result[\"source_documents\"]:\n",
        "        st.markdown(\n",
        "            f\"<div style='font-size:14px; line-height:1.4;'>- {doc.metadata['source']}</div>\",\n",
        "            unsafe_allow_html=True,\n",
        "        )\n",
        "\n",
        "#         # ê²°ê³¼ ì¶œë ¥\n",
        "# st.markdown(\"#### ğŸ“– ë‹µë³€:\")\n",
        "# st.markdown(f\"<div style='font-size:16px; line-height:1.6;'>{result['answer']}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "# st.markdown(\"#### ğŸ“š ì°¸ê³  ì¶œì²˜:\")\n",
        "# for doc in result[\"source_documents\"]:\n",
        "#     st.markdown(f\"<div style='font-size:14px; line-height:1.4;'>- {doc.metadata['source']}</div>\", unsafe_allow_html=True)\n"
      ],
      "metadata": {
        "id": "e7UsYrFsHywm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "pl-xgaD2KBrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ê°œì¸ í† í° ë²ˆí˜¸ ì…ë ¥\n",
        "!ngrok authtoken [ê°œì¸ í† í° ë²ˆí˜¸]"
      ],
      "metadata": {
        "id": "bifsVLSMKsFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!streamlit run app.py&>/dev/null&"
      ],
      "metadata": {
        "id": "CBFpB947KO7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "publ_url = ngrok.connect(addr=\"8501\")"
      ],
      "metadata": {
        "id": "xwHClxWMKX7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "publ_url"
      ],
      "metadata": {
        "id": "pE4A6SZdKcmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps"
      ],
      "metadata": {
        "id": "A9KYjg9iKmLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 13177 17180 12664"
      ],
      "metadata": {
        "id": "ss7eHUUKKkFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "NQPKytE6KnIk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}