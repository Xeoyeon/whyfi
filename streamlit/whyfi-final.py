import os
import random
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
import requests
import streamlit as st

from dotenv import load_dotenv
load_dotenv()

# Load Chroma VectorStore
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
vectorstore = Chroma(
    persist_directory="chroma_index", embedding_function=embedding_model
)

# Define retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Load Gemini
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

# Function to fetch news from Naver API
def fetch_naver_news(query):
    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")
    url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=50&start=2&sort=sim"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        news_items = response.json().get("items", [])
        if not news_items:
            return "ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        random_news = random.sample(news_items, min(3, len(news_items)))
        return "\n".join([f"- {item['title']} ({item['link']})" for item in random_news])
    
    return f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}"

# Template
template = """
ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê¸ˆìœµ ìš©ì–´ë¥¼ ëˆ„êµ¬ë‚˜ ì´í•´í•˜ê¸° ì‰½ê³  ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤. í•„ìš”í•˜ë©´ ì´í•´ë¥¼ ë•ê¸° ìœ„í•œ ì˜ˆì‹œë„ ì„¤ëª…ì— í¬í•¨í•©ë‹ˆë‹¤.
ê·¸ë¦¬ê³  ê·¸ ìš©ì–´ì™€ ì—°ê´€ëœ ê²€ìƒ‰ì–´ 3ê°œë¥¼ ì œê³µí•©ë‹ˆë‹¤. 

ê´€ë ¨ ì •ë³´:
{context}

ê¸ˆìœµ ìš©ì–´:
{term}

ğŸ’¡{term}ë€?: 

ğŸ”ì—°ê´€ ê²€ìƒ‰ì–´:
"""

# Generate prompt
prompt = PromptTemplate(

    input_variables=["context", "term"], 
    template=template
)

def format_retriever_output(docs):
    return "\n".join([doc.page_content for doc in docs])

# Generate Chain
chain = (
    {
        "context": retriever,
        "term": RunnablePassthrough(),
    }
    | prompt
    | llm 
    | StrOutputParser()
)

# Streamlit UI
st.set_page_config(page_title="ê¸ˆìœµ ìš©ì–´ ì•Œë¦¬ë¯¸", page_icon="ğŸ’°", layout="wide")

st.sidebar.title("ğŸ’° ê¸ˆìœµ ìš©ì–´ ì•Œë¦¬ë¯¸")
user_input = st.sidebar.text_input("ê¸ˆìœµ ìš©ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", "")

if user_input:
    st.sidebar.write("ğŸ” ê²€ìƒ‰ ì¤‘...")

    news_results = str(fetch_naver_news(user_input))
    retrieved_docs = retriever.invoke(user_input)
    context_text = "\n".join([doc.page_content for doc in retrieved_docs])

    with st.spinner("ğŸ”„ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘..."):
        response = chain.invoke(user_input)

    st.title("ğŸ“¢ ê¸ˆìœµ ìš©ì–´ ì„¤ëª…")
    st.markdown(response)

    st.subheader("ğŸ“° ê´€ë ¨ ë‰´ìŠ¤")
    if not news_results:
        st.write("âŒ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for news in news_results.split("\n"):
            st.markdown(news)

else:
    st.info("ğŸ” ê¸ˆìœµ ìš©ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”!")
